Gathering average execution times for systems. 


A100 GPT2 137M 512. 

--average time: 36.544061431884764 ms, 
            --calculated average time: 36.54406137063168 
            --forward time: 15.85426429748535 ms,
            --backward time: 20.620533714294435 ms,
            --optimizer time: 0.06162431880831718 ms,
            --scheduler time: 0.0037273600324988364 ms,
            --zero_grad time: 0.003911680011078715 ms,
            --file_name: torch_bitfit.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 38.611578674316405 ms, 
            --calculated average time: 38.61157900176011 
            --forward time: 17.586442337036132 ms,
            --backward time: 20.878643226623534 ms,
            --optimizer time: 0.13869055807590486 ms,
            --scheduler time: 0.003788800025358796 ms,
            --zero_grad time: 0.004014079999178648 ms,
            --file_name: torch_adapter.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 48.42280960083008 ms, 
            --calculated average time: 48.42280943986029 
            --forward time: 15.9010200881958 ms,
            --backward time: 26.046463928222657 ms,
            --optimizer time: 6.467952623367309 ms,
            --scheduler time: 0.0036864000372588634 ms,
            --zero_grad time: 0.0036864000372588634 ms,
            --file_name: torch_full.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 45.46433029174805 ms, 
            --calculated average time: 45.46433032084256
            --forward time: 19.67398910522461 ms,
            --backward time: 24.40003593444824 ms,
            --optimizer time: 0.7862886428833008 ms,
            --scheduler time: 0.021503999717533587 ms,
            --zero_grad time: 0.5825126385688781 ms,
            --file_name: torch_lora.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 42.969046936035156 ms, 
            --calculated average time: 42.9690473363176 
            --forward time: 24.760115394592287 ms,
            --backward time: 16.260280418395997 ms,
            --optimizer time: 1.3089382421970368 ms,
            --scheduler time: 0.021278719641268252 ms,
            --zero_grad time: 0.6184345614910126 ms,
            --file_name: exposer_bitfit.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 48.49358848571777 ms, 
            --calculated average time: 48.49358847405762 
            --forward time: 26.87940601348877 ms,
            --backward time: 19.8128231048584 ms,
            --optimizer time: 1.1264819157123567 ms,
            --scheduler time: 0.02138111975044012 ms,
            --zero_grad time: 0.6534963202476501 ms,
            --file_name: exposer_adapter.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 52.81757209777832 ms, 
            --calculated average time: 52.817571940235794 
            --forward time: 27.651625061035155 ms,
            --backward time: 24.070266876220703 ms,
            --optimizer time: 0.5770240020751953 ms,
            --scheduler time: 0.019005440287292005 ms,
            --zero_grad time: 0.4996505606174469 ms,
            --file_name: exposer_lora.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda

A100 GPT2 137M 1024. 

--average time: 63.83601676940918 ms, 
            --calculated average time: 63.83601666345261 
            --forward time: 33.194393692016604 ms,
            --backward time: 30.57264633178711 ms,
            --optimizer time: 0.061173759624361994 ms,
            --scheduler time: 0.003973120003938675 ms,
            --zero_grad time: 0.003829760020598769 ms,
            --file_name: exposer_bitfit.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 65.76101364135742 ms, 
            --calculated average time: 65.7610132331401 
            --forward time: 35.23997665405273 ms,
            --backward time: 30.37036521911621 ms,
            --optimizer time: 0.14284799993038177 ms,
            --scheduler time: 0.003870720034465194 ms,
            --zero_grad time: 0.003952640006318688 ms,
            --file_name: exposer_adapter.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 72.9012629699707 ms, 
            --calculated average time: 72.90126357164235 
            --forward time: 36.94284812927246 ms,
            --backward time: 34.588917846679685 ms,
            --optimizer time: 0.7382015979290009 ms,
            --scheduler time: 0.02129919972270727 ms,
            --zero_grad time: 0.6099967980384826 ms,
            --file_name: exposer_lora.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 83.89249069213867 ms, 
            --calculated average time: 83.892490495136 
            --forward time: 36.34348045349121 ms,
            --backward time: 47.39651596069336 ms,
            --optimizer time: 0.1448550409078598 ms,
            --scheduler time: 0.0037683200277388094 ms,
            --zero_grad time: 0.0038707200158387424 ms,
            --file_name: torch_adapter.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 84.27450347900391 ms, 
            --calculated average time: 84.27450389245524 
            --forward time: 35.171655807495114 ms,
            --backward time: 49.034915924072266 ms,
            --optimizer time: 0.060682240799069406 ms,
            --scheduler time: 0.0037273600324988364 ms,
            --zero_grad time: 0.0035225600562989712 ms,
            --file_name: torch_bitfit.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 93.81531616210937 ms, 
            --calculated average time: 93.81531652066856
            --forward time: 39.0041805267334 ms,
            --backward time: 53.41315071105957 ms,
            --optimizer time: 0.8067072010040284 ms,
            --scheduler time: 0.021852159835398198 ms,
            --zero_grad time: 0.5694259220361709 ms,
            --file_name: torch_lora.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 99.53218551635742 ms, 
            --calculated average time: 99.53218566088006 
            --forward time: 35.230371704101564 ms,
            --backward time: 57.845801162719724 ms,
            --optimizer time: 6.448680953979492 ms,
            --scheduler time: 0.003624960044398904 ms,
            --zero_grad time: 0.00370688003487885 ms,
            --file_name: torch_full.py, 
            --model: openai-community/gpt2, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda

A100 GPT2-XL 1.61B 512. 

--average time: 234.0199432373047 ms, 
            --calculated average time: 234.0199421630893 
            --forward time: 123.51498245239257 ms,
            --backward time: 110.27601379394531 ms,
            --optimizer time: 0.22167551666498184 ms,
            --scheduler time: 0.0036044800467789173 ms,
            --zero_grad time: 0.003665920039638877 ms,
            --file_name: exposer_bitfit.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 260.2553344726563 ms, 
            --calculated average time: 260.25533386516383 
            --forward time: 113.1192318725586 ms,
            --backward time: 146.9063983154297 ms,
            --optimizer time: 0.22247423708438874 ms,
            --scheduler time: 0.0035840000491589308 ms,
            --zero_grad time: 0.0036454400420188903 ms,
            --file_name: torch_bitfit.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 266.0228698730469 ms, 
            --calculated average time: 266.0228704191558 
            --forward time: 117.66259719848632 ms,
            --backward time: 147.56294586181642 ms,
            --optimizer time: 0.7896678388118744 ms,
            --scheduler time: 0.0038092800229787825 ms,
            --zero_grad time: 0.003850240018218756 ms,
            --file_name: torch_adapter.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 246.2791683959961 ms, 
            --calculated average time: 246.2791681584157 
            --forward time: 133.74453796386717 ms,
            --backward time: 111.73609451293946 ms,
            --optimizer time: 0.7911219215393066 ms,
            --scheduler time: 0.0037683200277388094 ms,
            --zero_grad time: 0.0036454400420188903 ms,
            --file_name: exposer_adapter.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 290.83983642578124 ms, 
            --calculated average time: 290.83983881354334 
            --forward time: 141.42046173095704 ms,
            --backward time: 144.86810668945313 ms,
            --optimizer time: 2.5607782411575317 ms,
            --scheduler time: 0.022056959867477417 ms,
            --zero_grad time: 1.9684351921081542 ms,
            --file_name: exposer_lora.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 317.50563720703127 ms, 
            --calculated average time: 317.50563989244404
            --forward time: 131.96740692138673 ms,
            --backward time: 181.3333203125 ms,
            --optimizer time: 2.6044620943069456 ms,
            --scheduler time: 0.02211839996278286 ms,
            --zero_grad time: 1.5783321642875672 ms,
            --file_name: torch_lora.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 381.8438250732422 ms, 
            --calculated average time: 381.84382471439426 
            --forward time: 113.308037109375 ms,
            --backward time: 188.6533837890625 ms,
            --optimizer time: 79.87472381591797 ms,
            --scheduler time: 0.0038092800229787825 ms,
            --zero_grad time: 0.0038707200158387424 ms,
            --file_name: torch_full.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 512, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda

A100 GPT2-XL 1.61B 1024. 

--average time: 397.0991522216797 ms, 
            --calculated average time: 397.09915070754477 
            --forward time: 183.07086334228515 ms,
            --backward time: 213.8008364868164 ms,
            --optimizer time: 0.22009855836629869 ms,
            --scheduler time: 0.0036044800467789173 ms,
            --zero_grad time: 0.003747840030118823 ms,
            --file_name: exposer_bitfit.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 406.17924743652344 ms, 
            --calculated average time: 406.1792463616282 
            --forward time: 190.80222778320314 ms,
            --backward time: 214.57833953857423 ms,
            --optimizer time: 0.7908966398239136 ms,
            --scheduler time: 0.003911680011078715 ms,
            --zero_grad time: 0.0038707200158387424 ms,
            --file_name: exposer_adapter.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 472.952626953125 ms, 
            --calculated average time: 472.95262678049505 
            --forward time: 209.1548681640625 ms,
            --backward time: 259.34045043945315 ms,
            --optimizer time: 2.4947916889190673 ms,
            --scheduler time: 0.02267136000096798 ms,
            --zero_grad time: 1.9398451280593871 ms,
            --file_name: exposer_lora.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 629.0448388671875 ms, 
            --calculated average time: 629.0448409416806 
            --forward time: 262.18094665527343 ms,
            --backward time: 366.6363000488281 ms,
            --optimizer time: 0.2198732775449753 ms,
            --scheduler time: 0.003829760020598769 ms,
            --zero_grad time: 0.003891200013458729 ms,
            --file_name: torch_bitfit.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 633.8401098632812 ms, 
            --calculated average time: 633.8401070852019 
            --forward time: 268.59085754394533 ms,
            --backward time: 364.44108825683594 ms,
            --optimizer time: 0.8004198443889617 ms,
            --scheduler time: 0.003911680011078715 ms,
            --zero_grad time: 0.003829760020598769 ms,
            --file_name: torch_adapter.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda
--average time: 707.47529296875 ms, 
            --calculated average time: 707.475289327465
            --forward time: 291.2561950683594 ms,
            --backward time: 412.2765509033203 ms,
            --optimizer time: 2.3872102427482607 ms,
            --scheduler time: 0.02166783984750509 ms,
            --zero_grad time: 1.5336652731895446 ms,
            --file_name: torch_lora.py, 
            --model: openai-community/gpt2-xl, 
            --batch size: 4, 
            --sequence length: 1024, 
            --data: dataset/train_gpt.jsonl, 
            --device: cuda

Gathering average execution times complete.

