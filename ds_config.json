{
    "train_batch_size": 8,
    "train_micro_batch_size_per_gpu": 8,
    "gradient_accumulation_steps": 1,

    "data_efficiency": {
        "seed": 0
    },
    
    "data_sampling":{
        "num_epochs": 5
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 5e-4,
            "weight_decay": 0.01
        }
    },

    "zero_optimization": {
        "stage": 0
    },

    "scheduler": {
         "type": "WarmupCosineLR",
         "params": {
             "total_num_steps": "auto",
             "warmup_num_steps": "auto"
         }
    },

    "note": "To add data offloading or no?"
}

